{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTI Tutorial: Using Automatic Mixed Precision and Model Compilation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dummy dataset\n",
    "dim = 512\n",
    "sequence_length = 128\n",
    "batch_size = 32\n",
    "num_batches = 100\n",
    "\n",
    "dummy_data = torch.randn(num_batches * batch_size, sequence_length, dim).to(device)\n",
    "dummy_target = torch.randint(0, 2, (num_batches * batch_size, sequence_length)).to(device)\n",
    "\n",
    "dataset = TensorDataset(dummy_data, dummy_target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Transformer model\n",
    "nlayers, nheads = 8, 8\n",
    "model = nn.Sequential(*[nn.TransformerEncoderLayer(d_model=dim, nhead=nheads) for _ in range(nlayers)])\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training time with and without AMP\n",
    "\n",
    "def eval_training_time(use_amp=False):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type, enabled=use_amp, dtype=torch.bfloat16):\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    toks_per_sec = num_batches * batch_size * sequence_length / run_time\n",
    "    return run_time, toks_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time without AMP: 4.66 seconds (87840.74 tokens/s)\n",
      "Training time with AMP (bfloat16): 1.39 seconds (294713.14 tokens/s)\n",
      "Speedup: 3.36x\n"
     ]
    }
   ],
   "source": [
    "# Measure training time and tokens per second\n",
    "time_standard, toks_per_sec_standard = eval_training_time(use_amp=False)\n",
    "time_amp, toks_per_sec_amp = eval_training_time(use_amp=True)\n",
    "\n",
    "print(f\"Training time without AMP: {time_standard:.2f} seconds ({toks_per_sec_standard:.2f} tokens/s)\")\n",
    "print(f\"Training time with AMP (bfloat16): {time_amp:.2f} seconds ({toks_per_sec_amp:.2f} tokens/s)\")\n",
    "print(f\"Speedup: {time_standard / time_amp:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model compilation with `torch.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_training_time(use_amp=False, compile_model=False):\n",
    "    if compile_model:\n",
    "        model_ = compiled_model\n",
    "    else:\n",
    "        model_ = model\n",
    "\n",
    "    model_.train()\n",
    "    start_time = time.time()\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type, enabled=use_amp, dtype=torch.bfloat16):\n",
    "            output = model_(data)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    toks_per_sec = num_batches * batch_size * sequence_length / run_time\n",
    "    return run_time, toks_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma2393/.conda/envs/computation_graph/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time without Compilation or AMP: 4.58 seconds (89409.98 tokens/s)\n",
      "Training time with AMP but no Compilation: 1.29 seconds (316841.28 tokens/s)\n",
      "Training time with Compilation but no AMP: 4.25 seconds (96439.70 tokens/s)\n",
      "Training time with AMP and Compilation: 1.00 seconds (408656.09 tokens/s)\n",
      "\n",
      "Speedup with AMP: 3.54x\n",
      "Speedup with Compilation: 1.08x\n",
      "Speedup with AMP and Compilation: 4.57x\n"
     ]
    }
   ],
   "source": [
    "# Measure training time and tokens per second\n",
    "\n",
    "# run compiled_model once to remove overhead\n",
    "_, _ = eval_training_time(use_amp=False, compile_model=True)\n",
    "_, _ = eval_training_time(use_amp=True, compile_model=True)\n",
    "\n",
    "time_standard, toks_per_sec_standard = eval_training_time(use_amp=False, compile_model=False)\n",
    "time_amp, toks_per_sec_amp = eval_training_time(use_amp=True, compile_model=False)\n",
    "time_comp, toks_per_sec_comp = eval_training_time(use_amp=False, compile_model=True)\n",
    "time_comp_amp, toks_per_sec_comp_amp = eval_training_time(use_amp=True, compile_model=True)\n",
    "\n",
    "print(f\"Training time without Compilation or AMP: {time_standard:.2f} seconds ({toks_per_sec_standard:.2f} tokens/s)\")\n",
    "print(f\"Training time with AMP but no Compilation: {time_amp:.2f} seconds ({toks_per_sec_amp:.2f} tokens/s)\")\n",
    "print(f\"Training time with Compilation but no AMP: {time_comp:.2f} seconds ({toks_per_sec_comp:.2f} tokens/s)\")\n",
    "print(f\"Training time with AMP and Compilation: {time_comp_amp:.2f} seconds ({toks_per_sec_comp_amp:.2f} tokens/s)\")\n",
    "print()\n",
    "print(f\"Speedup with AMP: {time_standard / time_amp:.2f}x\")\n",
    "print(f\"Speedup with Compilation: {time_standard / time_comp:.2f}x\")\n",
    "print(f\"Speedup with AMP and Compilation: {time_standard / time_comp_amp:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
